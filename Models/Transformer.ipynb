{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use Transformer MODEL\n",
    "\n",
    "- first install and import everithings\n",
    "- set the paths\n",
    "- execute everthing in saving end splitting data\n",
    "- then you can exucute the treining\n",
    "\n",
    "the saving and splitting data part need to be executed only one time just to create the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"dataset_different_size.csv\" #path to the CSV file containing the \"sequence\" and the \"label\"\n",
    "savepath = \"Dataset\" # you will nead a folder named Dataset containing tree empty folders named ids, att_mask and labels. Assaign to this var the path to the Dataset folder.\n",
    "\n",
    "#Once you crete the dataset you need to create other two folder named Dataset_validation and Dataset_Testing both containing tree empty folders named ids, att_mask and labels.\n",
    "\n",
    "training_path=\"Dataset\"    # equal to savepath\n",
    "validation_path=\"\"  # path of Dataset_validation\n",
    "testing_path=\"\"     # path of Dataset_testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "Positional Encoding is used to inject the position information of each token in the input sequence. It uses sine and cosine functions of different frequencies to generate the positional encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module): #subclass of nn.Module allowing it to be used as a pytorch layer\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        #d_model, the dimension of the model's input \n",
    "        #max_seq_lenght, maximun lenght of the input sequence \n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        #tensor setted equal to zero that will be populated with pos encodings\n",
    "        pe = torch.zeros(max_seq_length, d_model) \n",
    "        \n",
    "        #torch.arange create a tensor of index from 0 up to max lenght\n",
    "        #with unsqueeze we add a dimension so the tensor will pass from [max_len] to [max_len, 1]\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        #A term used to scale the position indices in a specific way.\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term) #sine to even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) #cosine to the odd indices\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0)) \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #The forward method simply adds the positional encodings to the input x.\n",
    "\n",
    "        #It uses the first x.size(1) elements of pe to ensure that the positional \n",
    "        # encodings match the actual sequence length of x.\n",
    "        \n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PositionalEncoding class adds information about the position of tokens within the sequence. Since the transformer model lacks inherent knowledge of the order of tokens (due to its self-attention mechanism), this class helps the model to consider the position of tokens in the sequence. The sinusoidal functions used are chosen to allow the model to easily learn to attend to relative positions, as they produce a unique and smooth encoding for each position in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forward Networks\n",
    "In summary, the PositionWiseFeedForward class defines a position-wise feed-forward neural network that consists of two linear layers with a ReLU activation function in between. In the context of transformer models, this feed-forward network is applied to each position separately and identically. It helps in transforming the features learned by the attention mechanisms within the transformer, acting as an additional processing step for the attention outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        #definition of the 2 linear transformation layer  \n",
    "        self.fc1 = nn.Linear(d_model, d_ff) \n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head Attention\n",
    "The Multi-Head Attention mechanism computes the attention between each pair of positions in a sequence. It consists of multiple “attention heads” that capture different aspects of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        #The initialization checks if d_model is divisible by num_heads, \n",
    "        #and then defines the transformation weights for query, key, value, and output.\n",
    "                \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation layer\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation layer\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation layer \n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation layer\n",
    "        #obviously considering the size of the input\n",
    "        \n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None): #here i need to apply the attention mask\n",
    "        # Calculate attention scores\n",
    "        # attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k). \n",
    "        # Here, the attention scores are calculated by taking the dot product of queries (Q) and keys (K), \n",
    "        # and then scaling by the square root of the key dimension (d_k).\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided \n",
    "        # (useful for preventing attention to certain parts like padding)\n",
    "        # this is foundamental in our specific use case since we are working with sequence with different lenght\n",
    "        # so we must use the attention mask to use only the actual attention score refering the actual lenght\n",
    "        if mask is not None:\n",
    "            #print(mask.shape)\n",
    "            #print(attn_scores.shape)\n",
    "            # Reshape mask to broadcast along dimensions 2 and 3\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)  # Add two singleton dimensions\n",
    "\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        #This method reshapes the input x into the shape (batch_size, num_heads, seq_length, d_k). \n",
    "        # It enables the model to process multiple attention heads concurrently, allowing for parallel computation.\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        # After applying attention to each head separately, this method combines the results back into \n",
    "        # a single tensor of shape (batch_size, seq_length, d_model). This prepares the result for further processing.\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer\n",
    "\n",
    "we create the Encoder layer in the classic way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics.classification\n",
    "\n",
    "\n",
    "class Transformer(pl.LightningModule):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        # The constructor takes the following parameters:\n",
    "\n",
    "        # src_vocab_size: Source vocabulary size.\n",
    "        # tgt_vocab_size: Target vocabulary size.\n",
    "        # d_model: The dimensionality of the model's embeddings.\n",
    "        # num_heads: Number of attention heads in the multi-head attention mechanism.\n",
    "        # num_layers: Number of layers for both the encoder and the decoder.\n",
    "        # d_ff: Dimensionality of the inner layer in the feed-forward network.\n",
    "        # max_seq_length: Maximum sequence length for positional encoding.\n",
    "        # dropout: Dropout rate for regularization.\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "          \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # for validation/testing\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.f1=torchmetrics.classification.BinaryF1Score()\n",
    "        self.precision=torchmetrics.classification.BinaryPrecision()\n",
    "        self.recall=torchmetrics.classification.BinaryRecall()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt, src_mask):\n",
    "\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        #print(\"Output shape pre fc:\" + str(enc_output.shape))\n",
    "        cls_output = enc_output[:,0]\n",
    "\n",
    "        #print(\"CLS output shape: \"+str(cls_output.shape))\n",
    "\n",
    "        output = self.fc(cls_output)\n",
    "        #print(\"Output shape after:\" + str(output.shape))\n",
    "\n",
    "        output = self.output_activation(output.squeeze())\n",
    "\n",
    "    \n",
    "        return output\n",
    "    \n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "      return F.binary_cross_entropy(logits, labels)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y, att = train_batch\n",
    "        \n",
    "        logits = self.forward(x,y,att)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y, att = val_batch\n",
    "        logits = self.forward(x,y,att)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        acc = self.accuracy(logits,y)\n",
    "        f1_val=self.f1(logits,y)\n",
    "        precision_val=self.precision(logits,y)\n",
    "        recall_val=self.recall(logits,y)\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_accuracy', acc)\n",
    "        self.log('val_f1', f1_val)\n",
    "        self.log('val_precision', precision_val)\n",
    "        self.log('val_recall', recall_val)\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y, att = test_batch\n",
    "        logits = self.forward(x,y,att)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        acc = self.accuracy(logits,y)\n",
    "        f1_test=self.f1(logits,y)\n",
    "        precision_test=self.precision(logits,y)\n",
    "        recall_test=self.recall(logits,y)\n",
    "\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_f1', f1_test)\n",
    "        self.log('test_accuracy', acc)\n",
    "        self.log('test_precision', precision_test)\n",
    "        self.log('test_recall', recall_test)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "      optimizer = torch.optim.AdamW(self.parameters(), lr=0.00001)\n",
    "      return optimizer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 1\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 500\n",
    "dropout = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", cls_token=\"[token]\",  trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sequence = self.df[\"sequence\"][index]\n",
    "        sequence = \"[token]\" + sequence #adding the cls_token for classification purpose\n",
    "        label = self.df[\"label\"][index]\n",
    "        return sequence, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Create custom dataset object\n",
    "train_data_object = CustomDataSet(datapath)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data_object,\n",
    "        batch_size=32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i,item in enumerate(train_loader):\n",
    "  dna,label = item\n",
    "  inputs = tokenizer(dna, return_tensors = 'pt', add_special_tokens=False, padding=True)\n",
    "  \n",
    "  ids = inputs[\"input_ids\"]\n",
    "  att_mask = inputs[\"attention_mask\"]\n",
    "  \n",
    "  torch.save(ids, savepath + \"/ids/%d.pt\" % i)\n",
    "  torch.save(att_mask, savepath + \"/att_mask/%d.pt\" % i)\n",
    "  torch.save(label, savepath + \"/labels/%d.pt\" % i )\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codice utilizzato per splittare il dataset in Training, Validation e Testing in rapporto 80/10/10\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "training_ids_path = os.path.join(training_path, \"ids\")\n",
    "training_att_mask_path = os.path.join(training_path, \"att_mask\")\n",
    "training_labels_path = os.path.join(training_path, \"labels\")\n",
    "\n",
    "validation_ids_path = os.path.join(validation_path, \"ids\")\n",
    "validation_att_mask_path = os.path.join(validation_path, \"att_mask\")\n",
    "validation_labels_path = os.path.join(validation_path, \"labels\")\n",
    "\n",
    "testing_ids_path = os.path.join(testing_path, \"ids\")\n",
    "testing_att_mask_path = os.path.join(testing_path, \"att_mask\")\n",
    "testing_labels_path = os.path.join(testing_path, \"labels\")\n",
    "\n",
    "\n",
    "for i, el in enumerate(os.listdir(training_ids_path)):\n",
    "    if i % 10 == 0:\n",
    "        shutil.move(os.path.join(training_ids_path, el), validation_ids_path)\n",
    "        shutil.move(os.path.join(training_att_mask_path, el), validation_att_mask_path)\n",
    "        shutil.move(os.path.join(training_labels_path, el), validation_labels_path)\n",
    "    if i % 10 == 1:\n",
    "        shutil.move(os.path.join(training_ids_path, el), testing_ids_path)\n",
    "        shutil.move(os.path.join(training_att_mask_path, el), testing_att_mask_path)\n",
    "        shutil.move(os.path.join(training_labels_path, el), testing_labels_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.df_ids = os.listdir(path+'/ids')\n",
    "        self.df_att_mak = os.listdir(path+'/att_mask')\n",
    "        self.df_labels = os.listdir(path+'/labels')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ids = torch.load(self.path+'/ids/'+self.df_ids[index])\n",
    "        att_mask = torch.load(self.path+'/att_mask/'+self.df_att_mak[index])\n",
    "        label = torch.load(self.path+'/labels/'+self.df_labels[index]).float()\n",
    "        print(\"loading file\"+self.path+'/ids/'+self.df_ids[index])\n",
    "\n",
    "        return ids,label,att_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create custom dataset object\n",
    "train_data_object = MyDataSet(DataTrainpath)\n",
    "test_data_object = MyDataSet(DataTestpath)\n",
    "val_data_object = MyDataSet(DataValpath)\n",
    "\n",
    "def collate(batch): #\n",
    "  (a, b, c) = batch[0]\n",
    "  return (a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataModule(pl.LightningDataModule):\n",
    "\n",
    "  def setup(self, stage):\n",
    "    self.dataset = \"\"#MyDataSet(\"\")\n",
    "\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return torch.utils.data.DataLoader(train_data_object,\n",
    "        batch_size=1, shuffle = False, collate_fn=collate)\n",
    "  def val_dataloader(self):\n",
    "    return torch.utils.data.DataLoader(val_data_object,\n",
    "        batch_size=1, shuffle = False, collate_fn=collate)\n",
    "  def test_dataloader(self):\n",
    "    return torch.utils.data.DataLoader(test_data_object,\n",
    "       batch_size=1, shuffle = False, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = pl.Trainer(max_epochs=4)\n",
    "data_module = MyDataModule()\n",
    "trainer.fit(transformer, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics= trainer.test(transformer, data_module)\n",
    "print(\"Loss on Test Set:\", metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioInfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
