{"cells":[{"cell_type":"markdown","metadata":{"id":"IRc_PXr8tWbS"},"source":["in questo caso invece di utilizzare l'architettura i Bert per realizzare gli embedding utilizziamo un approccio più semplice per effettuare la classificazione.\n","Ogni possibile coppia di basi viene mappata in quetso modo A = {1,0,0,0}, T = {0,1,0,0}, C = {0,0,1,0} e G = {0,0,0,1} in questo modo ogni sequenza lunga X viene mappata in un tensore con shape [X, 4] e viene dato come input ad una CNN per effettuare la classificazione"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"skW_T0_ktWbV"},"outputs":[],"source":["import torch\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P10LyHTItWbX"},"outputs":[],"source":["from torch.utils.data import Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vx7d2Xf9tWbY"},"outputs":[],"source":["datapath = \"/home/antoniodeblasi/Scaricati/data.csv\"\n","savepath = \"/home/antoniodeblasi/Scaricati/Dataset_1_hot\"\n","DataTrainpath=\"/home/antoniodeblasi/Scaricati/Dataset_1_hot\"\n","DataTestpath=\"/home/antoniodeblasi/Scaricati/Dataset_1_hot_validation\"\n","DataValpath=\"/home/antoniodeblasi/Scaricati/Dataset_1_hot_testing\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_z2Qii2itWbY"},"outputs":[],"source":["class CustomDataSet(Dataset):\n","    def __init__(self, csv_file):\n","        self.df = pd.read_csv(csv_file)\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","\n","    def __getitem__(self, index):\n","        sequence = self.df[\"sequenza\"][index]\n","        label = self.df[\" id\"][index]\n","        return sequence, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eO-SbEVmtWbZ"},"outputs":[],"source":["# Create custom dataset object\n","train_data_object = CustomDataSet(datapath)\n","\n","train_loader = torch.utils.data.DataLoader(train_data_object,\n","        batch_size=32, shuffle = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R6-Dcqe4tWbZ"},"outputs":[],"source":["\n","for i,item in enumerate(train_loader):\n","  dna,label = item\n","\n","# [32]--> [32,599,4]\n","  my_tensor = torch.zeros(label.shape[0],599,4)\n","  for j,elem in enumerate(dna):\n","      for k,c in enumerate(elem):\n","          if c == \"A\":\n","              my_tensor[j,k,0] = 1\n","          if c == \"T\":\n","              my_tensor[j,k,1] = 1\n","          if c == \"G\":\n","              my_tensor[j,k,2] = 1\n","          if c == \"C\":\n","              my_tensor[j,k,3] = 1\n","\n","\n","\n","\n","  torch.save(my_tensor, savepath + \"/embeddings/%d.pt\" % i)\n","  torch.save(label, savepath + \"/labels/%d.pt\" % i )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O7SbHX6GtWba"},"outputs":[],"source":["import numpy as np\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-psnB7V5tWba"},"outputs":[],"source":["import pytorch_lightning as pl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FnFgqSxItWbb"},"outputs":[],"source":["import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1LDA5T9htWbb"},"outputs":[],"source":["\n","class MyDataSet(Dataset):\n","    def __init__(self, path):\n","        self.path = path\n","        self.df_sequences = os.listdir(path+'/embeddings')\n","        self.df_labels = os.listdir(path+'/labels')\n","\n","    def __len__(self):\n","        return len(self.df_sequences)\n","\n","    def __getitem__(self, index):\n","        sequence = torch.load(self.path+'/embeddings/'+self.df_sequences[index])\n","        sequence = sequence.swapaxes(1,2)\n","\n","        label = torch.load(self.path+'/labels/'+self.df_labels[index]).float()\n","\n","\n","        return sequence, label\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nEYmTQsStWbb"},"outputs":[],"source":["\n","\n","\n","# Create custom dataset object\n","train_data_object = MyDataSet(DataTrainpath)\n","test_data_object = MyDataSet(DataTestpath)\n","val_data_object = MyDataSet(DataValpath)\n","\n","def collate(batch):\n","  (a, b) = batch[0]\n","  return (a,b)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BBYs733DtWbc"},"outputs":[],"source":["\n","class MyDataModule(pl.LightningDataModule):\n","\n","  def setup(self, stage):\n","    self.dataset = \"\"#MyDataSet(\"\")\n","\n","\n","  def train_dataloader(self):\n","    return torch.utils.data.DataLoader(train_data_object,\n","        batch_size=1, shuffle = False, collate_fn=collate)\n","  def val_dataloader(self):\n","    return torch.utils.data.DataLoader(val_data_object,\n","        batch_size=1, shuffle = False, collate_fn=collate)\n","  def test_dataloader(self):\n","    return torch.utils.data.DataLoader(test_data_object,\n","       batch_size=1, shuffle = False, collate_fn=collate)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_p-3haNtWbc"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchmetrics\n","\n","\n","class CNN(pl.LightningModule):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv1d(in_channels=4, out_channels=30, kernel_size=19, padding=\"same\")\n","        self.conv2 = nn.Conv1d(in_channels=30, out_channels=128, kernel_size=5, padding=\"same\")\n","\n","        self.pool = nn.MaxPool1d(10,stride=10)\n","\n","\n","        self.fc1 = nn.Linear(640, 513)\n","        self.fc2 = nn.Linear(513, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","        # for validation/testing\n","        self.accuracy = torchmetrics.Accuracy(task=\"binary\")\n","        self.f1=torchmetrics.classification.BinaryF1Score()\n","        self.precision=torchmetrics.classification.BinaryPrecision()\n","        self.recall=torchmetrics.classification.BinaryRecall()\n","\n","    def forward(self, x):\n","        # print(x.shape)\n","        x = torch.relu(self.conv1(x))\n","        # print(x.shape)\n","        x = self.pool(x)\n","        # print(x.shape)\n","        x = torch.relu(self.conv2(x))\n","        # print(x.shape)\n","        x = self.pool(x)\n","        # print(x.shape)\n","        x = torch.flatten(x,start_dim=1)\n","        # print(x.shape)\n","\n","        x = torch.relu(self.fc1(x))\n","        x = self.sigmoid(self.fc2(x))\n","        return x\n","\n","    def cross_entropy_loss(self, logits, labels):\n","      return F.binary_cross_entropy(logits, labels)\n","\n","\n","\n","\n","    def training_step(self, train_batch, batch_idx):\n","        x, y = train_batch\n","        logits = self.forward(x).squeeze()\n","        loss = self.cross_entropy_loss(logits, y)\n","        self.log('train_loss', loss)\n","        return loss\n","\n","\n","    def validation_step(self, val_batch, batch_idx):\n","        x, y = val_batch\n","        logits = self.forward(x).squeeze()\n","        loss = self.cross_entropy_loss(logits, y)\n","        acc = self.accuracy(logits,y)\n","        f1_val=self.f1(logits,y)\n","        precision_val=self.precision(logits,y)\n","        recall_val=self.recall(logits,y)\n","        self.log('val_loss', loss)\n","        self.log('val_accuracy', acc)\n","        self.log('val_f1', f1_val)\n","        self.log('val_precision', precision_val)\n","        self.log('val_recall', recall_val)\n","\n","    def test_step(self, test_batch, batch_idx):\n","        x, y = test_batch\n","        logits = self.forward(x).squeeze()\n","        loss = self.cross_entropy_loss(logits, y)\n","        acc = self.accuracy(logits, y)\n","        f1_test=self.f1(logits,y)\n","        precision_test=self.precision(logits,y)\n","        recall_test=self.recall(logits,y)\n","\n","        self.log('test_loss', loss)\n","        self.log('test_f1', f1_test)\n","        self.log('test_accuracy', acc)\n","        self.log('test_precision', precision_test)\n","        self.log('test_recall', recall_test)\n","        \n","    def configure_optimizers(self):\n","      optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n","      return optimizer\n","\n","# Creazione del modello\n","model = CNN()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eH7WqxA3tWbc","outputId":"91481214-b69a-44f6-cb98-ef79cb24ebf6"},"outputs":[{"name":"stderr","output_type":"stream","text":["GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"name":"stdout","output_type":"stream","text":["Testing DataLoader 0: 100%|██████████| 96/96 [00:00<00:00, 465.57it/s]\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5350130796432495     </span>│\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6921928524971008     </span>│\n","└───────────────────────────┴───────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5350130796432495    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6921928524971008    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name     | Type           | Params\n","--------------------------------------------\n","0 | conv1    | Conv1d         | 2.3 K \n","1 | conv2    | Conv1d         | 19.3 K\n","2 | pool     | MaxPool1d      | 0     \n","3 | fc1      | Linear         | 328 K \n","4 | fc2      | Linear         | 514   \n","5 | sigmoid  | Sigmoid        | 0     \n","6 | accuracy | BinaryAccuracy | 0     \n","--------------------------------------------\n","350 K     Trainable params\n","0         Non-trainable params\n","350 K     Total params\n","1.404     Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 79: 100%|██████████| 824/824 [00:02<00:00, 282.34it/s, v_num=60]      "]},{"name":"stderr","output_type":"stream","text":["`Trainer.fit` stopped: `max_epochs=80` reached.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 79: 100%|██████████| 824/824 [00:02<00:00, 281.77it/s, v_num=60]\n"]},{"name":"stderr","output_type":"stream","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"name":"stdout","output_type":"stream","text":["Testing DataLoader 0: 100%|██████████| 96/96 [00:00<00:00, 525.35it/s]\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8988874554634094     </span>│\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.7958797216415405     </span>│\n","└───────────────────────────┴───────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8988874554634094    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.7958797216415405    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Loss sul set di validazione: [{'test_loss': 1.7958797216415405, 'test_accuracy': 0.8988874554634094}]\n"]}],"source":["\n","torch.backends.cuda.matmul.allow_tf32 = True\n","\n","torch.backends.cudnn.allow_tf32 = True\n","trainer = pl.Trainer(max_epochs = 80)\n","\n","data_module = MyDataModule()\n","p = trainer.test(model, data_module)\n","\n","trainer.fit(model, data_module)\n","# Valutazione del modello\n","p = trainer.test(model, data_module)\n","print(\"Loss sul set di validazione:\", p)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
